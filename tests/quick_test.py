#!/usr/bin/env python3
"""
Quick test script for Model Demos functionality
"""

import sys
import os
from pathlib import Path

# Add components to path
components_path = str(Path(__file__).parent.parent / "components")
sys.path.insert(0, components_path)
print(f"Added to path: {components_path}")

def test_basic_functionality():
    """Test basic ModelDemos functionality"""
    print("ðŸ§ª Testing Model Demos Basic Functionality")
    print("=" * 50)
    
    try:
        # Test imports
        print("ðŸ“¦ Testing imports...")
        from model_demos import ModelDemos
        print("âœ… ModelDemos imported successfully")
        
        # Test initialization
        print("ðŸ”§ Testing initialization...")
        model_demos = ModelDemos()
        print("âœ… ModelDemos initialized successfully")
        
        # Test provider status
        print("ðŸ”Œ Testing provider status...")
        status = model_demos.get_provider_status()
        available = model_demos.get_available_providers()
        print(f"âœ… Provider status: {status}")
        print(f"âœ… Available providers: {available}")
        
        # Test basic classification
        print("ðŸ“Š Testing classification...")
        test_text = "AI system for automated loan approval based on credit scoring"
        prediction, scores, explanation = model_demos.classify_document(test_text)
        print(f"âœ… Classification result: {prediction}")
        print(f"âœ… Confidence scores: {len(scores)} classes")
        print(f"âœ… Explanation length: {len(explanation)} characters")
        
        # Test empty text handling
        print("ðŸ” Testing empty text handling...")
        pred_empty, scores_empty, exp_empty = model_demos.classify_document("")
        print(f"âœ… Empty text result: {pred_empty}")
        
        # Test rate limiting
        print("âš¡ Testing rate limiting...")
        limit_result = model_demos.check_rate_limit('test_provider')
        print(f"âœ… Rate limit check: {limit_result}")
        
        print("\nðŸŽ‰ All basic tests passed!")
        return True
        
    except Exception as e:
        print(f"âŒ Error in basic functionality test: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ai_integration():
    """Test AI provider integration"""
    print("\nðŸ¤– Testing AI Provider Integration")
    print("=" * 50)
    
    try:
        from model_demos import ModelDemos
        model_demos = ModelDemos()
        
        # Check API keys
        from dotenv import load_dotenv
        load_dotenv()
        
        api_keys = {
            'OpenAI': os.getenv('OPENAI_API_KEY'),
            'Mistral': os.getenv('MISTRAL_API_KEY'),
            'DeepSeek': os.getenv('DEEPSEEK_API_KEY')
        }
        
        print("ðŸ”‘ API Key Status:")
        available_providers = []
        for provider, key in api_keys.items():
            if key:
                print(f"  âœ… {provider}: Available")
                available_providers.append(provider.lower())
            else:
                print(f"  âŒ {provider}: Not configured")
        
        if available_providers:
            print(f"\nðŸ§  Testing AI analysis with {available_providers[0]}...")
            
            test_text = "AI system for customer service automation with natural language processing"
            
            # Test governance analysis
            result = model_demos.analyze_with_ai(
                test_text, 
                available_providers[0], 
                'governance'
            )
            
            if "API Error" in result or "not configured" in result:
                print(f"âš ï¸  API call failed: {result}")
            else:
                print(f"âœ… AI analysis completed successfully!")
                print(f"ðŸ“„ Result length: {len(result)} characters")
                print(f"ðŸ“„ Result preview: {result[:200]}...")
        else:
            print("âš ï¸  No API keys configured, skipping AI integration tests")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error in AI integration test: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance():
    """Test performance metrics"""
    print("\nðŸ“Š Testing Performance")
    print("=" * 50)
    
    try:
        import time
        from model_demos import ModelDemos
        
        model_demos = ModelDemos()
        
        # Test classification performance
        test_texts = [
            "AI system for automated loan approval",
            "Chatbot for customer service",
            "Facial recognition for security",
            "Product recommendation engine",
            "Medical diagnosis AI system"
        ]
        
        start_time = time.time()
        
        for i, text in enumerate(test_texts):
            classification_start = time.time()
            prediction, scores, explanation = model_demos.classify_document(text)
            classification_end = time.time()
            
            time_taken = (classification_end - classification_start) * 1000
            print(f"  {i+1}. {text[:40]}... -> {prediction} ({time_taken:.2f}ms)")
        
        end_time = time.time()
        total_time = (end_time - start_time) * 1000
        avg_time = total_time / len(test_texts)
        
        print(f"\nðŸ“ˆ Performance Results:")
        print(f"  Total time: {total_time:.2f}ms")
        print(f"  Average time per classification: {avg_time:.2f}ms")
        print(f"  Throughput: {len(test_texts)/(total_time/1000):.2f} classifications/second")
        
        if avg_time < 100:
            print("  âœ… Performance: EXCELLENT")
        elif avg_time < 500:
            print("  ðŸŸ¡ Performance: GOOD")
        else:
            print("  âŒ Performance: NEEDS IMPROVEMENT")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error in performance test: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run all tests"""
    print("ðŸ¤– Model Demos Test Suite")
    print("=" * 50)
    
    results = []
    
    # Run tests
    results.append(("Basic Functionality", test_basic_functionality()))
    results.append(("AI Integration", test_ai_integration()))
    results.append(("Performance", test_performance()))
    
    # Summary
    print("\nðŸ“‹ Test Summary")
    print("=" * 50)
    
    for test_name, passed in results:
        status = "âœ… PASSED" if passed else "âŒ FAILED"
        print(f"{test_name}: {status}")
    
    all_passed = all(result[1] for result in results)
    
    if all_passed:
        print("\nðŸŽ‰ All tests passed!")
        return 0
    else:
        print("\nâŒ Some tests failed!")
        return 1

if __name__ == "__main__":
    sys.exit(main())
